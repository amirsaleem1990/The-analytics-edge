{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Predicting Earnings from Census Data](https://ocw.mit.edu/courses/sloan-school-of-management/15-071-the-analytics-edge-spring-2017/trees/assignment-4/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The United States government periodically collects demographic information by conducting a census.\n",
    "#### In this problem, we are going to use census information about an individual to predict how much a person earns -- in particular, whether the person earns more than $50,000 per year. This data comes from the UCI Machine Learning Repository.\n",
    "#### The file census (CSV - 3.3MB) contains 1994 census data for 31,978 individuals in the United States.\n",
    "#### The dataset includes the following 13 variables:\n",
    "- <font color='red'>age = the age of the individual in years</font><br>\n",
    "- <font color='red'>workclass = the classification of the individual's working status (does the person work for the federal government, work for the local government, work without pay, and so on)</font><br>\n",
    "- <font color='red'>education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on)</font><br>\n",
    "- <font color='red'>maritalstatus = the marital status of the individual</font><br>\n",
    "- <font color='red'>occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on)</font><br>\n",
    "- <font color='red'>relationship = relationship of individual to his/her household</font><br>\n",
    "- <font color='red'>race = the individual's race</font><br>\n",
    "- <font color='red'>sex = the individual's sex</font><br>\n",
    "- <font color='red'>capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price)</font><br>\n",
    "- <font color='red'>capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price)</font><br>\n",
    "- <font color='red'>hoursperweek = the number of hours the individual works per week</font><br>\n",
    "- <font color='red'>nativecountry = the native country of the individual</font><br>\n",
    "- <font color='red'>over50k = whether or not the individual earned more than \\\\$50,000 in 1994</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.1 - A Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's begin by building a logistic regression model to predict whether an individual's earnings are above \\\\$50,000 (the variable \"over50k\") using all of the other variables as independent variables. First, read the dataset census.csv into R.\n",
    "#### Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60\\% of the observations, while the testing set contains 40\\% of the observations.\n",
    "#### Next, build a logistic regression model to predict the dependent variable \"over50k\", using all of the other variables in the dataset as independent variables. Use the training set to build the model.\n",
    "#### Which variables are significant, or have factors that have at least one significant dummy variable? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.) Select all that apply.\n",
    "- <font color='red'>age</font> # Correct <br>\n",
    "- <font color='red'>workclass</font> # Correct <br>\n",
    "- <font color='red'>education</font> # Correct <br>\n",
    "- <font color='red'>maritalstatus</font> # Correct <br>\n",
    "- <font color='red'>occupation</font> # Correct <br>\n",
    "- <font color='red'>relationship</font> # Correct <br>\n",
    "- <font color='red'>race</font><br>\n",
    "- <font color='red'>sex</font> # Correct <br>\n",
    "- <font color='red'>capitalgain</font> # Correct <br>\n",
    "- <font color='red'>capitalloss</font> # Correct <br>\n",
    "- <font color='red'>hoursperweek</font> # Correct <br>\n",
    "- <font color='red'>nativecountry</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t31978 obs. of  13 variables:\n",
      " $ age          : int  39 50 38 53 28 37 49 52 31 42 ...\n",
      " $ workclass    : Factor w/ 9 levels \" ?\",\" Federal-gov\",..: 8 7 5 5 5 5 5 7 5 5 ...\n",
      " $ education    : Factor w/ 16 levels \" 10th\",\" 11th\",..: 10 10 12 2 10 13 7 12 13 10 ...\n",
      " $ maritalstatus: Factor w/ 7 levels \" Divorced\",\" Married-AF-spouse\",..: 5 3 1 3 3 3 4 3 5 3 ...\n",
      " $ occupation   : Factor w/ 15 levels \" ?\",\" Adm-clerical\",..: 2 5 7 7 11 5 9 5 11 5 ...\n",
      " $ relationship : Factor w/ 6 levels \" Husband\",\" Not-in-family\",..: 2 1 2 1 6 6 2 1 2 1 ...\n",
      " $ race         : Factor w/ 5 levels \" Amer-Indian-Eskimo\",..: 5 5 5 3 3 5 3 5 5 5 ...\n",
      " $ sex          : Factor w/ 2 levels \" Female\",\" Male\": 2 2 2 2 1 1 1 2 1 2 ...\n",
      " $ capitalgain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...\n",
      " $ capitalloss  : int  0 0 0 0 0 0 0 0 0 0 ...\n",
      " $ hoursperweek : int  40 13 40 40 40 40 16 45 50 40 ...\n",
      " $ nativecountry: Factor w/ 41 levels \" Cambodia\",\" Canada\",..: 39 39 39 39 5 39 23 39 39 39 ...\n",
      " $ over50k      : Factor w/ 2 levels \" <=50K\",\" >50K\": 1 1 1 1 1 1 1 2 2 2 ...\n"
     ]
    }
   ],
   "source": [
    "census  <- read.csv(\"../data/census.csv\")\n",
    "str(census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“glm.fit: fitted probabilities numerically 0 or 1 occurred”"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = over50k ~ ., family = binomial, data = Train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-3.2341  -0.5045  -0.1821  -0.0004   3.4323  \n",
       "\n",
       "Coefficients: (1 not defined because of singularities)\n",
       "                                           Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept)                              -7.664e+00  1.113e+00  -6.887 5.71e-12\n",
       "age                                       2.441e-02  2.280e-03  10.708  < 2e-16\n",
       "workclass Federal-gov                     1.146e+00  2.124e-01   5.397 6.80e-08\n",
       "workclass Local-gov                       5.010e-01  1.925e-01   2.603 0.009239\n",
       "workclass Never-worked                   -1.045e+01  1.142e+03  -0.009 0.992699\n",
       "workclass Private                         7.582e-01  1.712e-01   4.428 9.51e-06\n",
       "workclass Self-emp-inc                    9.394e-01  2.051e-01   4.580 4.64e-06\n",
       "workclass Self-emp-not-inc                4.050e-01  1.881e-01   2.153 0.031304\n",
       "workclass State-gov                       3.424e-01  2.083e-01   1.644 0.100219\n",
       "workclass Without-pay                    -1.467e+01  1.021e+03  -0.014 0.988539\n",
       "education 11th                            2.531e-01  2.803e-01   0.903 0.366515\n",
       "education 12th                            6.638e-01  3.706e-01   1.791 0.073302\n",
       "education 1st-4th                         1.068e-01  5.348e-01   0.200 0.841789\n",
       "education 5th-6th                        -3.110e-01  4.405e-01  -0.706 0.480196\n",
       "education 7th-8th                        -5.183e-01  3.295e-01  -1.573 0.115707\n",
       "education 9th                            -2.076e-01  3.675e-01  -0.565 0.572081\n",
       "education Assoc-acdm                      1.318e+00  2.445e-01   5.392 6.97e-08\n",
       "education Assoc-voc                       1.382e+00  2.334e-01   5.922 3.19e-09\n",
       "education Bachelors                       1.896e+00  2.186e-01   8.675  < 2e-16\n",
       "education Doctorate                       3.001e+00  3.059e-01   9.807  < 2e-16\n",
       "education HS-grad                         7.471e-01  2.126e-01   3.514 0.000442\n",
       "education Masters                         2.379e+00  2.337e-01  10.180  < 2e-16\n",
       "education Preschool                      -1.342e+01  3.729e+02  -0.036 0.971291\n",
       "education Prof-school                     2.705e+00  2.786e-01   9.711  < 2e-16\n",
       "education Some-college                    1.142e+00  2.157e-01   5.295 1.19e-07\n",
       "maritalstatus Married-AF-spouse           2.516e+00  7.102e-01   3.543 0.000396\n",
       "maritalstatus Married-civ-spouse          1.634e+00  3.758e-01   4.347 1.38e-05\n",
       "maritalstatus Married-spouse-absent      -1.676e-02  3.341e-01  -0.050 0.959996\n",
       "maritalstatus Never-married              -5.405e-01  1.214e-01  -4.453 8.46e-06\n",
       "maritalstatus Separated                   3.667e-02  2.117e-01   0.173 0.862463\n",
       "maritalstatus Widowed                     2.131e-01  2.103e-01   1.014 0.310724\n",
       "occupation Adm-clerical                   1.861e-01  1.345e-01   1.384 0.166449\n",
       "occupation Armed-Forces                   1.896e-01  2.462e+00   0.077 0.938623\n",
       "occupation Craft-repair                   6.655e-02  1.155e-01   0.576 0.564365\n",
       "occupation Exec-managerial                8.451e-01  1.182e-01   7.152 8.58e-13\n",
       "occupation Farming-fishing               -1.142e+00  2.018e-01  -5.657 1.54e-08\n",
       "occupation Handlers-cleaners             -3.591e-01  1.912e-01  -1.878 0.060425\n",
       "occupation Machine-op-inspct             -3.376e-01  1.466e-01  -2.303 0.021259\n",
       "occupation Other-service                 -7.553e-01  1.703e-01  -4.436 9.17e-06\n",
       "occupation Priv-house-serv               -1.399e+01  2.355e+02  -0.059 0.952642\n",
       "occupation Prof-specialty                 6.005e-01  1.273e-01   4.719 2.37e-06\n",
       "occupation Protective-serv                5.804e-01  1.791e-01   3.241 0.001193\n",
       "occupation Sales                          3.481e-01  1.219e-01   2.856 0.004287\n",
       "occupation Tech-support                   7.864e-01  1.624e-01   4.841 1.29e-06\n",
       "occupation Transport-moving                      NA         NA      NA       NA\n",
       "relationship Not-in-family               -5.759e-02  3.717e-01  -0.155 0.876859\n",
       "relationship Other-relative              -5.340e-01  3.618e-01  -1.476 0.139997\n",
       "relationship Own-child                   -1.143e+00  3.750e-01  -3.047 0.002311\n",
       "relationship Unmarried                   -3.075e-02  3.920e-01  -0.078 0.937463\n",
       "relationship Wife                         1.430e+00  1.421e-01  10.060  < 2e-16\n",
       "race Asian-Pac-Islander                   5.445e-01  3.699e-01   1.472 0.141010\n",
       "race Black                                3.823e-01  3.022e-01   1.265 0.205739\n",
       "race Other                               -3.414e-01  5.419e-01  -0.630 0.528685\n",
       "race White                                4.846e-01  2.868e-01   1.689 0.091126\n",
       "sex Male                                  9.201e-01  1.106e-01   8.319  < 2e-16\n",
       "capitalgain                               3.174e-04  1.411e-05  22.495  < 2e-16\n",
       "capitalloss                               6.576e-04  5.086e-05  12.928  < 2e-16\n",
       "hoursperweek                              3.147e-02  2.292e-03  13.731  < 2e-16\n",
       "nativecountry Canada                     -6.064e-01  1.025e+00  -0.592 0.554043\n",
       "nativecountry China                      -7.185e-01  1.036e+00  -0.693 0.488143\n",
       "nativecountry Columbia                   -1.508e+01  3.535e+02  -0.043 0.965978\n",
       "nativecountry Cuba                       -4.035e-01  1.054e+00  -0.383 0.701703\n",
       "nativecountry Dominican-Republic         -1.432e+01  3.071e+02  -0.047 0.962804\n",
       "nativecountry Ecuador                    -1.432e+00  1.504e+00  -0.952 0.341034\n",
       "nativecountry El-Salvador                -7.012e-01  1.088e+00  -0.644 0.519391\n",
       "nativecountry England                    -5.571e-01  1.040e+00  -0.536 0.591976\n",
       "nativecountry France                     -7.792e-01  1.157e+00  -0.673 0.500635\n",
       "nativecountry Germany                    -4.665e-01  1.025e+00  -0.455 0.648923\n",
       "nativecountry Greece                     -1.905e+00  1.243e+00  -1.532 0.125550\n",
       "nativecountry Guatemala                  -1.808e-01  1.245e+00  -0.145 0.884548\n",
       "nativecountry Haiti                      -1.042e+00  1.368e+00  -0.762 0.446179\n",
       "nativecountry Holand-Netherlands         -1.337e+01  2.400e+03  -0.006 0.995556\n",
       "nativecountry Honduras                   -1.664e+00  2.878e+00  -0.578 0.563126\n",
       "nativecountry Hong                       -8.631e-01  1.409e+00  -0.613 0.540203\n",
       "nativecountry Hungary                    -1.570e+01  1.055e+03  -0.015 0.988129\n",
       "nativecountry India                      -6.194e-01  1.008e+00  -0.615 0.538763\n",
       "nativecountry Iran                       -4.890e-01  1.068e+00  -0.458 0.647125\n",
       "nativecountry Ireland                    -4.670e-01  1.330e+00  -0.351 0.725511\n",
       "nativecountry Italy                      -1.129e-01  1.049e+00  -0.108 0.914240\n",
       "nativecountry Jamaica                    -1.283e+00  1.176e+00  -1.091 0.275138\n",
       "nativecountry Japan                       8.014e-01  1.154e+00   0.695 0.487330\n",
       "nativecountry Laos                       -2.924e-01  1.338e+00  -0.219 0.827011\n",
       "nativecountry Mexico                     -9.220e-01  9.918e-01  -0.930 0.352555\n",
       "nativecountry Nicaragua                  -7.002e-01  1.255e+00  -0.558 0.577043\n",
       "nativecountry Outlying-US(Guam-USVI-etc) -1.493e+01  7.206e+02  -0.021 0.983471\n",
       "nativecountry Peru                       -1.030e+00  1.484e+00  -0.694 0.487837\n",
       "nativecountry Philippines                -2.038e-01  9.693e-01  -0.210 0.833484\n",
       "nativecountry Poland                     -2.266e-01  1.078e+00  -0.210 0.833514\n",
       "nativecountry Portugal                   -6.948e-01  1.246e+00  -0.558 0.577074\n",
       "nativecountry Puerto-Rico                -9.917e-01  1.081e+00  -0.917 0.358947\n",
       "nativecountry Scotland                   -2.842e-01  1.360e+00  -0.209 0.834483\n",
       "nativecountry South                      -1.378e+00  1.065e+00  -1.294 0.195586\n",
       "nativecountry Taiwan                     -7.401e-02  1.087e+00  -0.068 0.945702\n",
       "nativecountry Thailand                   -7.377e-01  1.363e+00  -0.541 0.588380\n",
       "nativecountry Trinadad&Tobago            -1.615e+00  1.523e+00  -1.060 0.289025\n",
       "nativecountry United-States              -4.132e-01  9.544e-01  -0.433 0.665073\n",
       "nativecountry Vietnam                    -1.556e+01  3.174e+02  -0.049 0.960907\n",
       "nativecountry Yugoslavia                  1.364e+00  1.320e+00   1.033 0.301406\n",
       "                                            \n",
       "(Intercept)                              ***\n",
       "age                                      ***\n",
       "workclass Federal-gov                    ***\n",
       "workclass Local-gov                      ** \n",
       "workclass Never-worked                      \n",
       "workclass Private                        ***\n",
       "workclass Self-emp-inc                   ***\n",
       "workclass Self-emp-not-inc               *  \n",
       "workclass State-gov                         \n",
       "workclass Without-pay                       \n",
       "education 11th                              \n",
       "education 12th                           .  \n",
       "education 1st-4th                           \n",
       "education 5th-6th                           \n",
       "education 7th-8th                           \n",
       "education 9th                               \n",
       "education Assoc-acdm                     ***\n",
       "education Assoc-voc                      ***\n",
       "education Bachelors                      ***\n",
       "education Doctorate                      ***\n",
       "education HS-grad                        ***\n",
       "education Masters                        ***\n",
       "education Preschool                         \n",
       "education Prof-school                    ***\n",
       "education Some-college                   ***\n",
       "maritalstatus Married-AF-spouse          ***\n",
       "maritalstatus Married-civ-spouse         ***\n",
       "maritalstatus Married-spouse-absent         \n",
       "maritalstatus Never-married              ***\n",
       "maritalstatus Separated                     \n",
       "maritalstatus Widowed                       \n",
       "occupation Adm-clerical                     \n",
       "occupation Armed-Forces                     \n",
       "occupation Craft-repair                     \n",
       "occupation Exec-managerial               ***\n",
       "occupation Farming-fishing               ***\n",
       "occupation Handlers-cleaners             .  \n",
       "occupation Machine-op-inspct             *  \n",
       "occupation Other-service                 ***\n",
       "occupation Priv-house-serv                  \n",
       "occupation Prof-specialty                ***\n",
       "occupation Protective-serv               ** \n",
       "occupation Sales                         ** \n",
       "occupation Tech-support                  ***\n",
       "occupation Transport-moving                 \n",
       "relationship Not-in-family                  \n",
       "relationship Other-relative                 \n",
       "relationship Own-child                   ** \n",
       "relationship Unmarried                      \n",
       "relationship Wife                        ***\n",
       "race Asian-Pac-Islander                     \n",
       "race Black                                  \n",
       "race Other                                  \n",
       "race White                               .  \n",
       "sex Male                                 ***\n",
       "capitalgain                              ***\n",
       "capitalloss                              ***\n",
       "hoursperweek                             ***\n",
       "nativecountry Canada                        \n",
       "nativecountry China                         \n",
       "nativecountry Columbia                      \n",
       "nativecountry Cuba                          \n",
       "nativecountry Dominican-Republic            \n",
       "nativecountry Ecuador                       \n",
       "nativecountry El-Salvador                   \n",
       "nativecountry England                       \n",
       "nativecountry France                        \n",
       "nativecountry Germany                       \n",
       "nativecountry Greece                        \n",
       "nativecountry Guatemala                     \n",
       "nativecountry Haiti                         \n",
       "nativecountry Holand-Netherlands            \n",
       "nativecountry Honduras                      \n",
       "nativecountry Hong                          \n",
       "nativecountry Hungary                       \n",
       "nativecountry India                         \n",
       "nativecountry Iran                          \n",
       "nativecountry Ireland                       \n",
       "nativecountry Italy                         \n",
       "nativecountry Jamaica                       \n",
       "nativecountry Japan                         \n",
       "nativecountry Laos                          \n",
       "nativecountry Mexico                        \n",
       "nativecountry Nicaragua                     \n",
       "nativecountry Outlying-US(Guam-USVI-etc)    \n",
       "nativecountry Peru                          \n",
       "nativecountry Philippines                   \n",
       "nativecountry Poland                        \n",
       "nativecountry Portugal                      \n",
       "nativecountry Puerto-Rico                   \n",
       "nativecountry Scotland                      \n",
       "nativecountry South                         \n",
       "nativecountry Taiwan                        \n",
       "nativecountry Thailand                      \n",
       "nativecountry Trinadad&Tobago               \n",
       "nativecountry United-States                 \n",
       "nativecountry Vietnam                       \n",
       "nativecountry Yugoslavia                    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 18979  on 17217  degrees of freedom\n",
       "Residual deviance: 10834  on 17121  degrees of freedom\n",
       "AIC: 11028\n",
       "\n",
       "Number of Fisher Scoring iterations: 15\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(2000)\n",
    "library(caTools)\n",
    "spl  <- sample.split(census, SplitRatio = 0.6)\n",
    "Train <- subset(census, spl == TRUE)\n",
    "Test <- subset(census, spl == FALSE)\n",
    "model  <- glm(over50k ~ ., data= Train, family = binomial)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.2 - A Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the accuracy of the model on the testing set? Use a threshold of p = 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == :\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        \n",
       "         FALSE  TRUE\n",
       "   <=50K 10421   777\n",
       "   >50K   1439  2123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.849864498644986"
      ],
      "text/latex": [
       "0.849864498644986"
      ],
      "text/markdown": [
       "0.849864498644986"
      ],
      "text/plain": [
       "[1] 0.8498645"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predTest <- predict(model, newdata = Test, type=\"response\")\n",
    "table(Test$over50k, predTest >= 0.5)\n",
    "(10421+2123)/(10421+777+1439+2123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.3 - A Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the accuracy of the baseline model on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " <=50K   >50K \n",
       " 11198   3562 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.758672086720867"
      ],
      "text/latex": [
       "0.758672086720867"
      ],
      "text/markdown": [
       "0.758672086720867"
      ],
      "text/plain": [
       "[1] 0.7586721"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(Test$over50k)\n",
    "11198/(11198+3562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.4 - A Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the area under the curve (AUC) for the logistic regression model on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: gplots\n",
      "\n",
      "Attaching package: ‘gplots’\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    lowess\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.904943433590169"
      ],
      "text/latex": [
       "0.904943433590169"
      ],
      "text/markdown": [
       "0.904943433590169"
      ],
      "text/plain": [
       "[1] 0.9049434"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ROCR)\n",
    "ROCRpredTest = prediction(predTest, Test$over50k)\n",
    "(auc = as.numeric(performance(ROCRpredTest, \"auc\")@y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.1 - A CART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have just seen how a logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.\n",
    "#### Let us now build a CART model to predict \"over50k\". Use the training set to build the model, with all of the other variables as independent variables. Use the default parameters (with the default loss table), so don't set a value for minbucket or cp and do not specify a loss table. Remember to specify method=\"class\" as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.\n",
    "#### How many splits does the tree have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "cartModel <- rpart(over50k ~ ., data=Train, method=\"class\")\n",
    "prp(cartModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.2 - A CART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which variable does the tree split on at the first level (the very first split of the tree)?\n",
    "- <font color='red'>age</font><br>\n",
    "- <font color='red'>workclass</font><br>\n",
    "- <font color='red'>education</font><br>\n",
    "- <font color='red'>maritalstatus</font><br>\n",
    "- <font color='red'>occupation</font><br>\n",
    "- <font color='red'>relationship</font> # Correct <br>\n",
    "- <font color='red'>race</font><br>\n",
    "- <font color='red'>sex</font><br>\n",
    "- <font color='red'>capitalgain</font><br>\n",
    "- <font color='red'>capitalloss</font><br>\n",
    "- <font color='red'>hoursperweek</font><br>\n",
    "- <font color='red'>nativecountry</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.3 - A CART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which variables does the tree split on at the second level (immediately after the first split of the tree)? Select all that apply.\n",
    "- <font color='red'>age</font><br>\n",
    "- <font color='red'>workclass</font><br>\n",
    "- <font color='red'>education</font> # Correct <br>\n",
    "- <font color='red'>maritalstatus</font><br>\n",
    "- <font color='red'>occupation</font><br>\n",
    "- <font color='red'>relationship</font><br>\n",
    "- <font color='red'>race</font><br>\n",
    "- <font color='red'>sex</font><br>\n",
    "- <font color='red'>capitalgain</font> # Correct <br>\n",
    "- <font color='red'>capitalloss</font><br>\n",
    "- <font color='red'>hoursperweek</font><br>\n",
    "- <font color='red'>nativecountry</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2.4 - A CART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the accuracy of the model on the testing set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        PredictCart\n",
       "          <=50K  >50K\n",
       "   <=50K  10531   667\n",
       "   >50K    1672  1890"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.841531165311653"
      ],
      "text/latex": [
       "0.841531165311653"
      ],
      "text/markdown": [
       "0.841531165311653"
      ],
      "text/plain": [
       "[1] 0.8415312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PredictCart <- predict(cartModel, newdata = Test, type = \"class\")\n",
    "table(Test$over50k, PredictCart)\n",
    "(10531+1890) / (10531+667+1672+1890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3.1 - A Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before building a random forest model, we'll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression. For this reason, before continuing we will define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set. Do this by running the following commands in your R console (assuming your training set is called \"censusTrain\"):\n",
    "#### set.seed(1)\n",
    "#### trainSmall = censusTrain[sample(nrow(censusTrain), 2000), ]\n",
    "#### Let us now build a random forest model to predict \"over50k\", using the dataset \"trainSmall\" as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest \"can not handle categorical predictors with more than 32 categories\", re-build the model without the nativecountry variable as one of the independent variables.)\n",
    "#### Then, make predictions using this model on the entire test set. What is the accuracy of the model on the test set, using a threshold of 0.5? (Remember that you don't need a \"type\" argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that your accuracy might be different from the one reported here, since random forest models can still differ depending on your operating system, even when the random seed is set. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        predictionsForest\n",
       "          <=50K  >50K\n",
       "   <=50K  10315   883\n",
       "   >50K    1198  2364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.859010840108401"
      ],
      "text/latex": [
       "0.859010840108401"
      ],
      "text/markdown": [
       "0.859010840108401"
      ],
      "text/plain": [
       "[1] 0.8590108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "trainSmall = census[sample(nrow(census), 2000), ]\n",
    "set.seed(1)\n",
    "library(randomForest)\n",
    "trainSmall$over50k <- as.factor(trainSmall$over50k)\n",
    "Test$over50k <- as.factor(Test$over50k)\n",
    "censusForest  <- randomForest(over50k ~ ., data= trainSmall)#, nodesize = 25, ntree = 200\n",
    "predictionsForest = predict(censusForest, newdata= Test)\n",
    "table(Test$over50k, predictionsForest)\n",
    "(10315+2364) / (10315+883+1198+2364)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may be surprised that the test-set accuracy of the random forest model is lower than the test-set accuracies of some of the other, simpler models we've trained. This can occur when the parameters of the random forest have not been tuned via cross-validation and the default parameters are ill suited for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1 - Selecting cp by Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now conclude our study of this data set by looking at how CART behaves with different choices of its parameters.\n",
    "#### Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.092 in 0.01 increments, by using the following command:\n",
    "#### cartGrid = data.frame( .cp = seq(0.002,0.092,0.01))\n",
    "#### After you create the cartGrid variable, you can pass it on to the train function by letting tuneGrid = cartGrid.\n",
    "#### Also, remember to use the entire training set \"censusTrain\" when building this model. The train function might take some time to run.\n",
    "#### Which value of cp does the train function recommend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Attaching package: ‘ggplot2’\n",
      "\n",
      "The following object is masked from ‘package:randomForest’:\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CART \n",
       "\n",
       "17218 samples\n",
       "   12 predictor\n",
       "    2 classes: ' <=50K', ' >50K' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 15496, 15496, 15495, 15497, 15496, 15495, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  cp     Accuracy   Kappa    \n",
       "  0.002  0.8563712  0.5751312\n",
       "  0.012  0.8466732  0.5407577\n",
       "  0.022  0.8433049  0.5199841\n",
       "  0.032  0.8433049  0.5199841\n",
       "  0.042  0.8297742  0.4627558\n",
       "  0.052  0.8215250  0.4164289\n",
       "  0.062  0.8101415  0.2984142\n",
       "  0.072  0.7599605  0.0000000\n",
       "  0.082  0.7599605  0.0000000\n",
       "  0.092  0.7599605  0.0000000\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was cp = 0.002."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(caret)\n",
    "library(e1071)\n",
    "numFolds = trainControl( method = \"cv\", number = 10 )\n",
    "cartGrid <- data.frame( .cp = seq(0.002,0.092,0.01))\n",
    "set.seed(2)\n",
    "(cartCV  <- train(over50k ~ ., data = Train,method = \"rpart\", \n",
    "      trControl = numFolds, tuneGrid = cartGrid ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.2 - Selecting cp by Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall that the caret package causes categorical independent variables to be split into factor levels. To generate a version of the test set with dummy variables, please run the following command (which assumes your test set is stored in variable \"censusTest\"):\n",
    "#### censusTestMM = as.data.frame(model.matrix(over50k~.+0, data=censusTest))\n",
    "#### Use the final model selected via cross-validation to make predictions on censusTestMM. What is the prediction accuracy on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        predictTest\n",
       "          <=50K  >50K\n",
       "   <=50K  10490   708\n",
       "   >50K    1477  2085"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.851964769647697"
      ],
      "text/latex": [
       "0.851964769647697"
      ],
      "text/markdown": [
       "0.851964769647697"
      ],
      "text/plain": [
       "[1] 0.8519648"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model <- cartCV$final\n",
    "censusTestMM = as.data.frame(model.matrix(over50k~.+0, data=Test))\n",
    "predictTest = predict(model, newdata=censusTestMM, type=\"class\")\n",
    "table(Test$over50k, predictTest)\n",
    "(10490+2085)/(10490+708+1477+2085)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.3 - Selecting cp by Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compared to the original accuracy using the default value of cp, this new CART model is an improvement, and so we should clearly favor this new model over the old one -- or should we? Plot the CART tree for this model. How many splits are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "17"
      ],
      "text/latex": [
       "17"
      ],
      "text/markdown": [
       "17"
      ],
      "text/plain": [
       "[1] 17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(model$cptable[,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>This highlights one important tradeoff in building predictive models. By tuning cp, we improved our accuracy, but our tree became significantly more complex. In some applications, such an improvement in accuracy would be worth the loss in interpretability. In others, we may prefer a less accurate model that is simpler to understand and describe over a more accurate -- but more complicated -- model.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
